"""
Description
-----------
This script fine-tunes MedGemma-4B-IT on paired chest X-ray images and
radiology reports. DICOM images are loaded from disk, converted to
grayscale PIL images with VOI-LUT and percentile-based contrast
normalization. Reports are loaded from text files and used as targets
for supervised fine-tuning (findings-only style).

The training setup uses:
- A custom CXRReportDataset for image/report pairs
- A FindingsSFT wrapper that converts samples into model-ready tensors
- A VLMCollator that pads text sequences and stacks image tensors
- LoRA adapters on key projection layers for parameter-efficient tuning

Inputs
------
- CSVs with split annotations:
    - "dataset downloading/Dataset/cxr_reports_train_v2_isNone.csv"
    - "dataset downloading/Dataset/cxr_reports_val_v2_isNone.csv"
    - "dataset downloading/Dataset/cxr_reports_test_v2_isNone.csv"
  Each CSV is expected to contain at least:
    - subject_id, study_id, dcm_path, report_path, isNone, view_prompt

- DICOM images and reports rooted under:
    - "dataset downloading/Dataset/"

Outputs
-------
- Fine-tuned LoRA adapters in: "cxr_checkpoints/medgemma"
- Saved processor in the same directory

Notes
-----
- Rows with isNone == True are removed before building datasets.
  These *_isNone CSVs are generated by a separate script that iterates
  over all reports, checks the report content, and flags isNone=True
  when the report text contains "None" (no findings). Only rows with
  isNone == False are used here.
- Image transforms (resize, normalization) are left to the model
  processor. The dataset only produces grayscale PIL images.
"""

# ==============================================================
# Imports and global configuration
# ==============================================================

import os
from dataclasses import dataclass
from typing import List, Dict, Any

import numpy as np
import pandas as pd
import pydicom
import torch
from PIL import Image
from pydicom.pixel_data_handlers.util import apply_voi_lut
from torch.utils.data import Dataset

from transformers import (
    AutoProcessor,
    AutoModelForImageTextToText,
    TrainingArguments,
    Trainer,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# ==============================================================
# Dataset: DICOM + report loader
# ==============================================================

class CXRReportDataset(Dataset):
    """
    Dataset loader for paired (DICOM → image), (TXT → report).

    Output per sample
    -----------------
    {
        "image"      : PIL.Image in grayscale "L" (no transforms applied)
        "view_prompt": str, e.g., "[VIEW=PA]"
        "report"     : str (raw report text)
        "subject_id" : int
        "study_id"   : int
    }

    Notes
    -----
    - Does not resize or normalize beyond DICOM intensity normalization;
      the Hugging Face processor handles model-specific transforms.
    - Images are returned in 1-channel grayscale (mode="L").
    """

    def __init__(self, df: pd.DataFrame, debug: bool = False):
        """
        Parameters
        ----------
        df : pd.DataFrame
            DataFrame with columns:
            - dcm_path
            - report_path
            - view_prompt
            - subject_id
            - study_id
        debug : bool, optional
            If True, prints file paths as they are loaded.
        """
        self.df = df.reset_index(drop=True)
        self.debug = debug

    def load_dicom(self, path: str) -> Image.Image:
        """
        Reads a DICOM file, applies VOI-LUT (if available) and
        percentile contrast clipping, returns a PIL grayscale image.
        """
        dicom = pydicom.dcmread(path, force=True)
        data = dicom.pixel_array

        # Apply VOI LUT when windowing metadata is present
        if hasattr(dicom, "WindowWidth"):
            data = apply_voi_lut(data, dicom)

        data = data.astype(np.float32)

        # Contrast clipping to reduce extreme outliers
        low, high = np.percentile(data, (0.5, 99.5))
        data = np.clip(data, low, high)

        # Normalize to [0, 255] and convert to grayscale PIL image
        data = (data - data.min()) / (data.max() - data.min() + 1e-6)
        data = (data * 255).astype(np.uint8)
        return Image.fromarray(data).convert("L")

    def load_report(self, path: str) -> str:
        """
        Reads report text from disk (no cleaning here).

        Parameters
        ----------
        path : str
            Path to report file on disk.

        Returns
        -------
        str
            Raw report text as stored in the file.
        """
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read()

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        row = self.df.iloc[idx]

        # DICOM path is rooted under "dataset downloading/Dataset/"
        img_path = os.path.join("dataset downloading/Dataset/", row["dcm_path"])

        # Report path is stored in the CSV relative to the dataset root
        rpt_path = row["report_path"]

        # View prompt, e.g., "[VIEW=PA]" (default if missing)
        prompt = row.get("view_prompt", "[VIEW=UNKNOWN]")

        if self.debug:
            print(f"[DEBUG] DICOM:  {img_path}")
            print(f"[DEBUG] REPORT: {rpt_path}")

        image = self.load_dicom(img_path)
        report = self.load_report(rpt_path)

        return {
            "image": image,
            "view_prompt": prompt,
            "report": report,
            "subject_id": row["subject_id"],
            "study_id": row["study_id"],
        }

    def __len__(self) -> int:
        return len(self.df)


# ==============================================================
# Load Train / Val / Test splits and build datasets
# ==============================================================

TRAIN_CSV = "dataset downloading/Dataset/cxr_reports_train_v2_isNone.csv"
VAL_CSV   = "dataset downloading/Dataset/cxr_reports_val_v2_isNone.csv"
TEST_CSV  = "dataset downloading/Dataset/cxr_reports_test_v2_isNone.csv"

# Load split CSVs
train_df = pd.read_csv(TRAIN_CSV)
val_df   = pd.read_csv(VAL_CSV)
test_df  = pd.read_csv(TEST_CSV)

# Filter out rows where isNone == True (no findings text)
train_df = train_df[train_df["isNone"] == False].reset_index(drop=True)
val_df   = val_df[val_df["isNone"] == False].reset_index(drop=True)
test_df  = test_df[test_df["isNone"] == False].reset_index(drop=True)

print(f"Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}")

# Instantiate base datasets
train_set = CXRReportDataset(train_df)
val_set   = CXRReportDataset(val_df)
test_set  = CXRReportDataset(test_df)

print("Datasets loaded.")

# ==============================================================
# Model, processor, and LoRA configuration
# ==============================================================

MODEL_NAME = "google/medgemma-4b-it"
OUT_DIR    = "cxr_checkpoints/medgemma"
USE_4BIT   = False  # Set True if you later enable QLoRA

# Training hyperparameters
EPOCHS       = 1         # Single epoch as requested
BATCH_SIZE   = 4         # Per-device batch size
GRAD_ACCUM   = 8         # Effective batch = BATCH_SIZE * GRAD_ACCUM
LR           = 1e-4
WEIGHT_DECAY = 0.0
WARMUP_RATIO = 0.03
MAX_SEQ_LEN  = 2048

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

INSTRUCTION = (
    "You are a CXR analyst. Return only chest X-ray FINDINGS in 1–3 short sentences. "
    "No headings, no patient/date/indication, no “preliminary”, no disclaimers. "
    "Do not assume sex/age/view beyond provided text. Use precise radiology wording. "
    "If normal, write a single normal line (e.g., "
    "“Lungs clear. Cardiomediastinal silhouette normal. No pleural effusion or pneumothorax.”). "
    "If abnormal, state the key positives only, concise (e.g., "
    "“Feeding tube projects to upper abdomen; bibasilar atelectasis; "
    "no pneumothorax or pleural effusion.”). Start directly with the findings."
)

# Load processor and base model
processor = AutoProcessor.from_pretrained(MODEL_NAME)

model = AutoModelForImageTextToText.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
)

# Tokenizer and padding setup
tok = processor.tokenizer
if tok.pad_token_id is None:
    tok.pad_token = tok.eos_token
model.config.pad_token_id = tok.pad_token_id

# LoRA configuration on common projection layers
lora_cfg = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
)

if USE_4BIT:
    # Optional path if QLoRA is enabled later
    model = prepare_model_for_kbit_training(model)

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

# ==============================================================
# Prompt builder and example encoder
# ==============================================================

def build_prompt(view_prompt: str) -> str:
    """
    Builds a MedGemma chat-style prompt with one image slot and
    the findings-only instruction.

    Parameters
    ----------
    view_prompt : str
        View tag such as "[VIEW=PA]" or "[VIEW=UNKNOWN]".

    Returns
    -------
    str
        Rendered prompt string including the <image> placeholder.
    """
    messages = [{
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": f"{view_prompt} {INSTRUCTION}"},
        ],
    }]
    return processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=False,
    )


def encode_example(
    pil_L_image: Image.Image,
    view_prompt: str,
    target_text: str,
    max_len: int = MAX_SEQ_LEN,
) -> Dict[str, torch.Tensor]:
    """
    Encodes a single training example into tokenized text and image tensors.

    Parameters
    ----------
    pil_L_image : PIL.Image
        Grayscale image in mode "L".
    view_prompt : str
        View tag to prepend, e.g., "[VIEW=PA]".
    target_text : str
        Ground-truth report text (findings).
    max_len : int
        Maximum allowed sequence length for text tokens.

    Returns
    -------
    dict
        {
            "input_ids": LongTensor [T],
            "attention_mask": LongTensor [T],
            "labels": LongTensor [T] (prompt masked with -100),
            "pixel_values": FloatTensor [C,H,W]
        }
    """
    # Build prompt
    prompt = build_prompt(view_prompt)

    # Processor encodes prompt + image jointly
    enc = processor(
        text=prompt,
        images=[pil_L_image.convert("RGB")],
        return_tensors="pt",
        padding=False,
    )

    input_ids = enc["input_ids"][0]
    attention_mask = enc["attention_mask"][0]
    pixel_values = enc["pixel_values"][0]

    # Target tokens (findings only) with EOS
    target_ids = tok(
        target_text.strip(),
        add_special_tokens=False,
        return_tensors="pt",
    )["input_ids"][0]
    target_ids = torch.cat(
        [target_ids, torch.tensor([tok.eos_token_id], dtype=target_ids.dtype)],
        dim=0,
    )

    # Concatenate prompt and target
    full_input_ids = torch.cat([input_ids, target_ids], dim=0)
    full_attn_mask = torch.cat([attention_mask, torch.ones_like(target_ids)], dim=0)

    # Truncate if sequence exceeds max length
    if full_input_ids.size(0) > max_len:
        overflow = full_input_ids.size(0) - max_len
        full_input_ids = full_input_ids[overflow:]
        full_attn_mask = full_attn_mask[overflow:]

    # Build labels: mask prompt tokens with -100, keep target tokens
    labels = torch.full_like(full_input_ids, -100)
    tgt_len = min(target_ids.size(0), full_input_ids.size(0))
    labels[-tgt_len:] = full_input_ids[-tgt_len:]

    return {
        "input_ids": full_input_ids,
        "attention_mask": full_attn_mask,
        "labels": labels,
        "pixel_values": pixel_values,
    }


# ==============================================================
# SFT dataset wrapper and collator
# ==============================================================

class FindingsSFT(Dataset):
    """
    Wraps CXRReportDataset to produce model-ready (prompt, image, target) tensors.
    """
    def __init__(self, base_ds: Dataset):
        self.ds = base_ds

    def __len__(self) -> int:
        return len(self.ds)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        sample = self.ds[idx]
        return encode_example(
            pil_L_image=sample["image"],
            view_prompt=sample["view_prompt"],
            target_text=sample["report"],
        )


@dataclass
class VLMCollator:
    """
    Data collator for variable-length text and vision inputs.

    Pads token sequences to the max length in the batch and stacks
    pixel_values along the batch dimension.
    """
    pad_token_id: int

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_ids_list = [b["input_ids"] for b in batch]
        attn_mask_list = [b["attention_mask"] for b in batch]
        labels_list    = [b["labels"] for b in batch]
        pixel_values   = torch.stack([b["pixel_values"] for b in batch], dim=0)

        max_len = max(x.size(0) for x in input_ids_list)

        def pad(x: torch.Tensor, val: int) -> torch.Tensor:
            pad_len = max_len - x.size(0)
            if pad_len <= 0:
                return x
            return torch.cat(
                [x, torch.full((pad_len,), val, dtype=x.dtype)],
                dim=0,
            )

        input_ids = torch.stack(
            [pad(x, self.pad_token_id) for x in input_ids_list],
            dim=0,
        )
        attention_mask = torch.stack(
            [pad(x, 0) for x in attn_mask_list],
            dim=0,
        )
        labels = torch.stack(
            [pad(x, -100) for x in labels_list],
            dim=0,
        )

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "pixel_values": pixel_values,
        }


# Wrap base datasets with SFT dataset
train_sft = FindingsSFT(train_set)
val_sft   = FindingsSFT(val_set)

collate_fn = VLMCollator(pad_token_id=tok.pad_token_id)

print(f"SFT wrappers — Train={len(train_sft)}, Val={len(val_sft)}")

# ==============================================================
# Trainer setup
# ==============================================================

args = TrainingArguments(
    output_dir=OUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    learning_rate=LR,
    weight_decay=WEIGHT_DECAY,
    warmup_ratio=WARMUP_RATIO,
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    bf16=torch.cuda.is_available(),
    gradient_checkpointing=False,
    dataloader_num_workers=2,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_sft,
    eval_dataset=val_sft,
    data_collator=collate_fn,
)

# ==============================================================
# Train and save
# ==============================================================

if __name__ == "__main__":
    train_out = trainer.train()
    print(train_out)

    # Save LoRA adapters and processor
    trainer.model.save_pretrained(OUT_DIR)
    processor.save_pretrained(OUT_DIR)

    print(f"Saved LoRA adapters and processor to: {OUT_DIR}")
